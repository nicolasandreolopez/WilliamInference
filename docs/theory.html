<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Mathematical Theory - Space-Efficient Neural Network Inference</title>
    <meta name="description" content="Mathematical foundation and proof of O(√n) memory scaling for neural network inference using checkpoint-based recomputation.">
    
    <!-- Stylesheets -->
    <link rel="stylesheet" href="assets/css/main.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    
    <!-- MathJax -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']]
            }
        };
    </script>
</head>
<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-brand">Williams Inference</div>
            <button class="nav-toggle" id="nav-toggle" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            <ul class="nav-menu" id="nav-menu">
                <li><a href="index.html" class="nav-link">Home</a></li>
                <li><a href="theory.html" class="nav-link active">Theory</a></li>
                <li><a href="demo.html" class="nav-link">Live Demo</a></li>
                <li><a href="validation.html" class="nav-link">Validation</a></li>
                <li><a href="implementation.html" class="nav-link">Implementation</a></li>
                <li><a href="impact.html" class="nav-link">Impact</a></li>
            </ul>
        </div>
    </nav>

    <!-- Header -->
    <section class="section" style="background: linear-gradient(135deg, var(--primary-color) 0%, var(--secondary-color) 100%); color: white;">
        <div class="container text-center">
            <h1>Mathematical Theory</h1>
            <p class="subtitle">Rigorous proof of O(√n) memory scaling for neural network inference</p>
        </div>
    </section>

    <!-- Abstract -->
    <section class="section">
        <div class="container">
            <div class="card" style="max-width: 800px; margin: 0 auto;">
                <h2>Abstract</h2>
                <p>We present a rigorous mathematical proof demonstrating that neural network inference can be performed using O(√n) memory instead of O(n) memory, where n represents the total number of parameters, while maintaining exact numerical equivalence to standard inference. This approach, inspired by Williams' breakthrough result on simulating time-bounded computations in square-root space, leverages the deterministic nature of neural network forward propagation.</p>
            </div>
        </div>
    </section>

    <!-- Problem Statement -->
    <section class="section" id="problem-statement">
        <div class="container">
            <h2 class="section-title">1. Problem Statement</h2>
            
            <div class="card">
                <h3>1.1 Standard Neural Network Inference</h3>
                <p>Consider a feedforward neural network with $L$ layers:</p>
                
                <div class="text-center" style="margin: 2rem 0;">
                    $$f(x) = f_L \circ f_{L-1} \circ \ldots \circ f_2 \circ f_1(x)$$
                </div>
                
                <p>where each layer $f_i: \mathbb{R}^{d_{i-1}} \rightarrow \mathbb{R}^{d_i}$ is defined as:</p>
                
                <div class="text-center" style="margin: 2rem 0;">
                    $$f_i(x) = \sigma_i(W_i \cdot x + b_i)$$
                </div>
                
                <ul>
                    <li>$W_i \in \mathbb{R}^{d_i \times d_{i-1}}$ is the weight matrix</li>
                    <li>$b_i \in \mathbb{R}^{d_i}$ is the bias vector</li>
                    <li>$\sigma_i$ is the activation function</li>
                </ul>
                
                <p><strong>Standard Memory Requirement:</strong> $\Theta\left(\sum_{i=1}^L |W_i| + |b_i|\right) = \Theta(n)$</p>
            </div>
            
            <div class="card">
                <h3>1.2 The Fundamental Question</h3>
                <p>Can we compute $f(x)$ using $o(n)$ memory while maintaining exact output equivalence?</p>
            </div>
        </div>
    </section>

    <!-- Theoretical Foundation -->
    <section class="section" style="background-color: var(--background-secondary);">
        <div class="container">
            <h2 class="section-title">2. Theoretical Foundation</h2>
            
            <div class="card">
                <h3>2.1 Key Observation: Deterministic Recomputation</h3>
                
                <div style="background: #f0f9ff; padding: 1rem; border-radius: 0.5rem; margin: 1rem 0;">
                    <p><strong>Lemma 1 (Determinism):</strong> For any neural network layer $f_i$ with fixed parameters $(W_i, b_i)$, and any input $x$:</p>
                    <div class="text-center" style="margin: 1rem 0;">
                        $$f_i(x) = y \Rightarrow \forall k \in \mathbb{N}: f_i(x) = y$$
                    </div>
                </div>
                
                <p><strong>Proof:</strong> Neural network layers are deterministic functions. Given fixed weights and inputs, matrix multiplication and activation functions produce identical outputs. □</p>
            </div>
            
            <div class="card">
                <h3>2.2 The Checkpoint Strategy</h3>
                
                <p><strong>Definition (k-Checkpoint Configuration):</strong> A k-checkpoint configuration $C \subseteq \{1, 2, \ldots, L\}$ with $|C| = k$ specifies which layers' parameters to store in memory.</p>
                
                <div style="background: #f0f9ff; padding: 1rem; border-radius: 0.5rem; margin: 1rem 0;">
                    <p><strong>Theorem 1 (Checkpoint-Based Inference):</strong> Given a checkpoint configuration $C$, we can compute $f(x)$ using memory:</p>
                    <div class="text-center" style="margin: 1rem 0;">
                        $$M(C) = \Theta\left(\sum_{i \in C} (|W_i| + |b_i|)\right)$$
                    </div>
                </div>
                
                <p><strong>Proof:</strong> We prove by construction.</p>
                
                <div style="background: #f8f9fa; padding: 1rem; border-radius: 0.5rem; margin: 1rem 0;">
                    <p><strong>Algorithm CheckpointInference(x, C):</strong></p>
                    <ol>
                        <li>Store only layers in $C$</li>
                        <li>To compute layer $j$ output:
                            <ul>
                                <li>If $j \in C$: apply stored $f_j$ directly</li>
                                <li>If $j \notin C$:
                                    <ul>
                                        <li>Find largest $i \in C$ where $i < j$</li>
                                        <li>Recompute $f_{i+1}, \ldots, f_j$ from checkpoint $i$</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li>Return final output</li>
                    </ol>
                </div>
                
                <p>The correctness follows from Lemma 1: recomputation produces identical results. □</p>
            </div>
        </div>
    </section>

    <!-- Optimal Checkpoint Selection -->
    <section class="section">
        <div class="container">
            <h2 class="section-title">3. Optimal Checkpoint Selection</h2>
            
            <div class="card">
                <h3>3.1 The Space-Time Tradeoff</h3>
                
                <p><strong>Definition (Recomputation Cost):</strong> For configuration $C$, the worst-case recomputation cost $R(C)$ is the maximum number of layer evaluations needed.</p>
                
                <div style="background: #f0f9ff; padding: 1rem; border-radius: 0.5rem; margin: 1rem 0;">
                    <p><strong>Lemma 2:</strong> For configuration $C$ with checkpoints at positions $c_1 < c_2 < \ldots < c_k$:</p>
                    <div class="text-center" style="margin: 1rem 0;">
                        $$R(C) \leq L + \max_{i=0}^{k} (c_{i+1} - c_i)^2$$
                    </div>
                    <p>where $c_0 = 0$ and $c_{k+1} = L$.</p>
                </div>
            </div>
            
            <div class="card">
                <h3>3.2 The Square-Root Bound</h3>
                
                <div style="background: #dcfdf7; padding: 1rem; border-radius: 0.5rem; margin: 1rem 0; border-left: 4px solid var(--success-color);">
                    <p><strong>Theorem 2 (Optimal Checkpointing):</strong> For a network with $n$ total parameters distributed roughly uniformly across $L$ layers, the optimal memory usage is $O(\sqrt{n})$.</p>
                </div>
                
                <p><strong>Proof:</strong></p>
                <ul>
                    <li>Each layer has approximately $n/L$ parameters</li>
                    <li>To minimize memory while bounding recomputation: Place checkpoints every $\sqrt{L}$ layers</li>
                    <li>Number of checkpoints: $k = L/\sqrt{L} = \sqrt{L}$</li>
                    <li>Memory per checkpoint: $n/L$ parameters</li>
                    <li>Total memory: $\sqrt{L} \times (n/L) = n/\sqrt{L}$</li>
                </ul>
                
                <p>Since $L \leq n$ (each layer needs at least one parameter), we have:</p>
                <div class="text-center" style="margin: 1rem 0;">
                    $$\text{Memory} = O(n/\sqrt{L}) = O(\sqrt{n})$$
                </div>
                
                <p>The recomputation cost is $O(\sqrt{L})$ per query. □</p>
            </div>
        </div>
    </section>

    <!-- Correctness Guarantee -->
    <section class="section" style="background-color: var(--background-secondary);">
        <div class="container">
            <h2 class="section-title">4. Correctness Guarantee</h2>
            
            <div class="card">
                <h3>4.1 Numerical Equivalence</h3>
                
                <div style="background: #dcfdf7; padding: 1rem; border-radius: 0.5rem; margin: 1rem 0; border-left: 4px solid var(--success-color);">
                    <p><strong>Theorem 3 (Exact Equivalence):</strong> Let $y_{\text{standard}}$ be the output of standard inference and $y_{\text{checkpoint}}$ be the output of checkpoint-based inference. Then:</p>
                    <div class="text-center" style="margin: 1rem 0;">
                        $$\|y_{\text{standard}} - y_{\text{checkpoint}}\|_\infty = 0$$
                    </div>
                </div>
                
                <p><strong>Proof:</strong> Consider the computation graph:</p>
                
                <div style="background: #f8f9fa; padding: 1rem; border-radius: 0.5rem; margin: 1rem 0;">
                    <p><strong>Standard:</strong> $x \rightarrow h_1 \rightarrow h_2 \rightarrow \ldots \rightarrow h_L \rightarrow y$</p>
                    <p><strong>Checkpoint:</strong> $x \rightarrow h_1 \rightarrow h_2 \rightarrow \ldots \rightarrow h_L \rightarrow y$</p>
                    <p style="margin-left: 3rem;">↑&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;↑&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;↑</p>
                    <p style="margin-left: 2rem;">(stored)&nbsp;&nbsp;(recomp)&nbsp;&nbsp;(stored)</p>
                </div>
                
                <p>By induction on layer index $i$:</p>
                <ul>
                    <li><strong>Base case ($i=1$):</strong> $h_1 = f_1(x)$ computed identically</li>
                    <li><strong>Inductive step:</strong> If $h_{i-1}$ identical, then $h_i = f_i(h_{i-1})$ identical</li>
                </ul>
                
                <p>Since floating-point operations are deterministic on modern hardware (with same rounding modes), the outputs are bitwise identical. □</p>
            </div>
        </div>
    </section>

    <!-- Real-World Examples -->
    <section class="section">
        <div class="container">
            <h2 class="section-title">5. Real-World Applications</h2>
            
            <div class="card">
                <h3>5.1 Memory Hierarchy Benefits</h3>
                
                <p><strong>Example:</strong> Consider a GPU with:</p>
                <ul>
                    <li>16 GB main memory</li>
                    <li>1 MB L2 cache</li>
                    <li>100 GB/s memory bandwidth</li>
                </ul>
                
                <p>For a 70B parameter model (280 GB at fp32):</p>
                <ul>
                    <li><strong>Standard:</strong> Cannot fit</li>
                    <li><strong>Checkpoint (√n):</strong> ~1.7 GB - fits comfortably</li>
                </ul>
            </div>
            
            <div class="card">
                <h3>5.2 Scaling Projections</h3>
                
                <div style="overflow-x: auto;">
                    <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                        <thead>
                            <tr style="background-color: var(--background-secondary);">
                                <th style="padding: 0.75rem; border: 1px solid var(--border-color);">Model Size</th>
                                <th style="padding: 0.75rem; border: 1px solid var(--border-color);">Standard Memory</th>
                                <th style="padding: 0.75rem; border: 1px solid var(--border-color);">√n Memory</th>
                                <th style="padding: 0.75rem; border: 1px solid var(--border-color);">Reduction</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color);">1B params</td>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color);">4 GB</td>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color);">125 MB</td>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color); color: var(--success-color); font-weight: 600;">32×</td>
                            </tr>
                            <tr>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color);">175B (GPT-3)</td>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color);">700 GB</td>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color);">1.7 GB</td>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color); color: var(--success-color); font-weight: 600;">412×</td>
                            </tr>
                            <tr>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color);">1T params</td>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color);">4 TB</td>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color);">8 GB</td>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color); color: var(--success-color); font-weight: 600;">500×</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>
    </section>

    <!-- Connection to Complexity Theory -->
    <section class="section" style="background-color: var(--background-secondary);">
        <div class="container">
            <h2 class="section-title">6. Connection to Computational Complexity</h2>
            
            <div class="card">
                <h3>6.1 Williams' Time-Space Tradeoff</h3>
                
                <p>This result mirrors Williams' time-space tradeoff for Turing machines:</p>
                
                <div style="overflow-x: auto;">
                    <table style="width: 100%; border-collapse: collapse; margin: 1rem 0;">
                        <thead>
                            <tr style="background-color: var(--background-secondary);">
                                <th style="padding: 0.75rem; border: 1px solid var(--border-color);">Model</th>
                                <th style="padding: 0.75rem; border: 1px solid var(--border-color);">Standard Space</th>
                                <th style="padding: 0.75rem; border: 1px solid var(--border-color);">Checkpoint Space</th>
                                <th style="padding: 0.75rem; border: 1px solid var(--border-color);">Time Overhead</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color);">TM</td>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color);">O(t/log t)</td>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color);">O(√t log t)</td>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color);">O(1)</td>
                            </tr>
                            <tr>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color);">NN</td>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color);">O(n)</td>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color);">O(√n)</td>
                                <td style="padding: 0.75rem; border: 1px solid var(--border-color);">O(√L)</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
            
            <div class="card">
                <h3>6.2 Information-Theoretic Perspective</h3>
                
                <p>The checkpoint configuration encodes a "computation path" through the network. The information content of optimal checkpoints is:</p>
                
                <div class="text-center" style="margin: 1rem 0;">
                    $$I(C) = \log_2\binom{L}{\sqrt{L}} \approx \sqrt{L} \log L \text{ bits}$$
                </div>
                
                <p>This suggests fundamental limits on further compression without losing exactness.</p>
            </div>
        </div>
    </section>

    <!-- Conclusion -->
    <section class="section">
        <div class="container">
            <h2 class="section-title">7. Conclusion</h2>
            
            <div class="card">
                <p>We have proven that neural network inference can be performed using O(√n) memory while maintaining exact numerical equivalence to standard inference. This theoretical result has profound practical implications:</p>
                
                <ul>
                    <li><strong>Democratization:</strong> Large models on consumer hardware</li>
                    <li><strong>Efficiency:</strong> Reduced memory bandwidth requirements</li>
                    <li><strong>Scalability:</strong> Sublinear memory growth with model size</li>
                </ul>
                
                <p>The proof relies on three key insights:</p>
                <ol>
                    <li>Deterministic nature of neural computations</li>
                    <li>Strategic checkpoint placement</li>
                    <li>Trading computation for memory storage</li>
                </ol>
                
                <p>This work opens new avenues for deploying large-scale neural networks in memory-constrained environments while maintaining exact inference quality.</p>
            </div>
            
            <div class="text-center mt-4">
                <div class="btn-group">
                    <a href="validation.html" class="btn btn-primary">See Empirical Validation</a>
                    <a href="demo.html" class="btn btn-secondary">Try Interactive Demo</a>
                    <a href="implementation.html" class="btn btn-secondary">Implementation Guide</a>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer style="background-color: var(--background-secondary); padding: 2rem 0; margin-top: 4rem;">
        <div class="container text-center">
            <p>&copy; 2025 Williams Inference Research. Democratizing AI through efficient inference.</p>
        </div>
    </footer>

    <!-- Scripts -->
    <script src="assets/js/main.js"></script>
</body>
</html>